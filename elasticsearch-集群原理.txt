集群原理
    集群命令
        节点操作
            集群的分类
            集群的状态监控
                集群状态
                    state
                集群统计
                    stats
                集群任务管理
                    tasks
                节点信息
                    nodes
                活跃线程信息
                    nodes/hot_threads
            集群的备份
                snapshot
                restore
    监控集群
        概述
        这个怎么运作
        生产环境监控
        使用 Metricbeat 收集监控数据
        使用 Filebeat 收集日志数据
        配置监控索引
        遗留收集方法
        故障排除
    组成
        主节点作用
            维护并更新cluster状态
            节点的加入集群及移除
            分片的平衡
        协调节点
    分布式特性
        分片
            Primary Shard 主分片
                将数据分布到所有节点之上
                7.0默认主分片为1
                主分片将一份索引的数据分散在多个Data Node上实现水平扩展
                主分片在索引创建的时候指定，后续默认不能修改，如果要修改，需要重建索引
                主分片过少
                    例如只有一个分片，如果索引数据增长很快，集群无法通过增加节点实现对这个索引的数据扩展(就一个分片咋扩展啊)
                        所以数据量过大时，要重建索引，扩展节点
                主分片过多
                    单个分片容量很小，导致一个节点上有过多分片，影响性能
            Replica Shard 副本分片
                副本分片保证高可用
                    可动态调整
                    副本分片提升系统读取性能  通过增加replica个数 一定程度可以提高读取的吞吐量
                副本分片的引入提高了数据可用性
                    如果不设置副本分片那可能丢失数据
                    从主写 可服务读
                副本分片提升系统读取性能
                    通过增加replica个数 一定程度可以提高读取的吞吐量
                        请求会转发到不同节点 找副本分片或主分片 减低负载
                        还是要结合物理资源的 你不能说你一台机器搞那么多分片 那也不会提升
                        请求会广播到所有分片  合并数据
                        https://elasticsearch.cn/question/3028
                        https://www.elastic.co/guide/en/elasticsearch/guide/current/_query_phase.html
                7.0默认副本分片为0
                副本分片过多：降低集群整体写入性能
            分片过少
                无法水平扩展
                单个分片数据量太大
                    数据重新分配耗时
            7.0 默认只有一个分片
            提前做好容量规划
            分片过多
                影响搜索打分，统计准确性
                    文档在每个分片上打好分汇总到一起的
                浪费资源，影响性能
            保证高可用和分布式（扩展）
                划分分片
        集群
            状态
                黄
                    部分副本分片没完全分配
                    部分副本分片不可用
                红
                    主分片没完全分配
                    部分主分片不可使用
            不同的集群通过不同的名字来区分 通过配置文件设定
            跨集群搜索
                在单集群情况下，水平扩展时，节点数不能无限增加当集群的meta信息过多导致更新压力变大单个active master会成为性能瓶颈导致整个集群无法正常工作
                5.3引入跨集群搜索功能 cross cluster search
                    允许任何节点扮演federated节点以轻量的方式将搜索请求代理
                    不需要以client node的形式加入其他集群
            节点
                节点本质就是一个java进程 生产上最好一台机器一个节点每一个节点都有名字 通过配置文件配置每个节点启动后会分配一个UID 保存在data目录下
                master node
                    一个集群中只有一个master
                    只有master可以修改集群中必要信息
                    其他节点保持一致
                    处理创建删除索引请求/决定分片被分配到哪个节点/负责索引的创建与删除
                    维护并更新cluster state
                    非常重要 部署上要考虑解决单点问题
                    唯一个集群设置多个master eligible 节点
                        等着只承担master单一角色
                master eligible node
                    每个节点启动后就是一个这样的节点
                    可通过设置node:master:false让它无法成为master
                    这样的节点有权被选举成master
                    默认第一个启动的成为master
                data node
                    存储数据的节点
                    默认是数据节点
                    保存分片的数据，起到了数据扩展的作用
                    master node决定了如何把分片分发到数据节点上
                    解决数据水平扩展和数据单点的问题
                coordinating node
                    协调节点
                    路由请求到正确节点 例如创建索引请求需要路由到master节点接收请求，分发请求，汇总结果
                    默认都是协调节点
                hot&amp;warm node
                machine learning node
                节点设置
                ingest node
                    预处理节点
                        相比logstash处理数据功能较简单局限性高，但部署起来也简单
                        作为文档索引中的一环，用来处理数据，再将文档索引进es
                    https://cloud.tencent.com/developer/article/1354027
                生产中尽量让一个节点 单一职责
                节点1 master true 其他false节点2 data true 其他false节点3 协调节点节点 1 2 3 都可以访问 一位每个节点都是一个协调节点
            集群状态
                所有节点信息
                所有的索引和其相关的mapping与setting信息
                分片的路由信息
                每个节点上都保存了集群的状态信息
                只有master节点才能修改集群的状态信息并负责同步给其他节点
            选主流程！！！！
                互相ping对方 id的当选主节点
                选主的过程应该很短，这个期间，如果有创建index或者分片reallocation有可能会出错
            集群脑裂
                集群隔开后重新选举了主节点
                7.0开始无需配置
            故障转移
                通过增加删除节点
                    集群可能会再次重新在节点上分配分片
                一个主分片不可用，只要设置了副本分片，其中一个副本分片立即会将自己提升为主分片。同时会将自己的数据分配到一个新的replica上
                有时候，我们只是对集群中一台机器重启，没必要做故障转移
                    可以让这个reallocation延迟一段时间
                    避免无谓的数据拷贝
                数据会根据文档id并结合相应的hash算法将数据分发到不同的分片，即不同的shard上的数据肯定是不一样的
                数据量不大，p和r上的数据应该会很快一致，数据量很小，数据从p到r需要很久，需要检查集群是否存在性能问题
                故障转移期间，如果只是黄色变绿，应该不影响读写，因为副本会提升为主分片。集群变红，代表有主分片丢失，这个时候会影响读写。
                数据丢失
                    node如果丢失，没有落盘，有丢失的可能。如果节点重新回来，会从translog中恢复没有写入的数据
        文档分布式存储
            文档如何均匀到分片上
            哈希路由算法
                shard= hash(_routing)%number_of_primary_shards
                确保了文档均匀分散到分片中
                默认routing是文档id
                可以自己指定routing
                由于公式中主分片的数量对公式结果的影响
                    所以这是主分片数量不能随意修改的根本原因
            更新一个文档
                请求到协调节点
                hash文档id寻找分片
                路由到对应分片
                先删后索引
                成功后返回给协调节点
                协调节点响应
                副本分片也会同步数据
            删除一个文档
                类似
            同步问题
                1、如果索引分片都可用，并且写入主副本都成功了，才返回给客户端。在refresh后，主副本同时对外提供查询服务。这个同步时间很短，可以认为主副本的数据是一致的。如果副本同步时间较长，甚至超过1s，在同步的时间差内，可能造成数据不一致的情况，但是对外的数据最终是一致的，所以是分布式下的最终一致性。2、如果索引主分片不可用，将禁止写入，数据是强一致的；3、如果索引副本分片不可用，则有部分副本写入失败，整体写入认为成功。由于不可用的副本也不提供查询服务，所以对外数据仍然是一致的。
            为什么不用一致性哈希算法
                简单的说就是一致性哈希算法 N/1数据移动就行
                但是es里面Lucene索引数据占很大比例，文档数据才占5%
                这就导致了数据移动时，删除重新索引特别耗费时间性能
                所以不合适
                但是如果你能保证你的索引是增量的，那么可以直接建新索引，用alias cover住旧索引和新索引
    集群原理
        es的分布式架构原理能说一下么（es是如何实现分布式的）
        es生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少分片？
            （1）es生产集群我们部署了5台机器，每台机器是6核64G的，集群总内存是320G（2）我们es集群的日增量数据大概是2000万条，每天日增量数据大概是500MB，每个月数据量大概在6亿，15G。       目前系统已经运行了几个月了，现在集群数据总量大概是100G左右。（3）目前线上有5个索引（结合业务来），每个索引的数据量大概是20G,所以这个数据量内，我们每个索引分配的是8个shard，比默认的5个shard多了3个shard。
        集群的故障检测
        集群处理并发冲突
            乐观锁
            集群确实主节点
        如何实现master选举
            ZenDiscover模块负责
            选主最少节点限制
                如果对某个节点的投票数达到一定值并且该节点自己也选举自己，那这个节点就是master
            脑裂问题