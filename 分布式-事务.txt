分布式事务
    事务
        分布式事务并发控制
            事务枷锁
            事务时间戳
            乐观并发控制
        事务与事务类型
            计算机事务处理
            事务串行化
            事务终止恢复
            事务分类
                刚性事务
                柔性事务（最大努力通知方案）：在线下棋
                    思想
                        使用消息中间件
                        不断发消息通知
                        例：订单业务
                    总结
                柔性事务（最终一致性方案）：滴滴通知
                柔性事务（TCC补偿性方案）：订婚领证
                    思想
                        Try：prepare行为，调用自定义的prepare逻辑
                        Confirm：commit行为
                        Cancel：rollback行为
                    总结
                        优势在于可以自定义数据操作的粒度
                        不足之处是对于应用的侵入性非常强
                        业务逻辑分支都需要try、confirm、cancel三步操作
                    TCC方案
                        用了补偿的概念
                            全程是：Try，Confirm，Cancel
                            1.Try阶段：对各个服务的资源做检测以及对资源进行锁定或预留
                            2.Confirm
                                这个阶段是在各个服务中执行实际的操作
                            3.Cancel
                                如果任何遗憾个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑回滚操作。
                        try阶段：先去冻结银行紫金
                        confirm阶段
                            调用银行B的接口，扣款。调用银行C的接口，转账
                                如果执行失败，进行Cancel阶段
                        Cancel阶段
                            通过跟业务相关的代码，将银行B扣的款给加回去
                        比较适合的场景：
                            这个除非你是真的要求一致性要求太高，是你系统中核心之核心的场景，跟钱相关的资金类的场景，那你可以用TCC方案，自己编写大量的业务逻辑，自己判断各个阶段是否OK，还得确保每个系统执行的时间都比较短。
                    XA方案 两阶段提交方案
                        比如团建
                            第一阶段：tb主席问每个人能不能去，如果能去，那么ok，一起去这次tb。如果这个阶段里，任何一个人回答说，去不了，那么tb主席取消这次活动
                            第二阶段：那下周六大家就一起去滑雪+烧烤了
                        事务管理器
                            相当于team building主席
                            第一阶段：询问
                                事务管理器去问每个系统，能不能执行这个操作？
                            第二阶段：执行
                                挨个通知各个系统进行数据处理
                            常见于协调一个系统需要对多个数据库进行写入，基于spring+JTA就可以搞定
                                这种分布式事务方案，比较适合单块应用，跨多个库的分布式事务，因为严重依赖于数据库层面来搞定复杂的事务
                            但这这种一个服务多个库不符合开发规范了
    问题解决方案
        分布式死锁检测
            思索检测消息
            集中式死锁检测
            死锁预防
        读写隔离
            1.写隔离
                一阶段本地事务提交前，需要确保先拿到 全局锁
                拿不到全局锁，不能提交本地事务
                拿全局锁的尝试被限制在一定范围内，超出范围将放弃，并回滚本地事务，释放本地锁。
                一阶段
                    tx1先开始，开启本地事务，拿到本地锁，更新操作m = 1000-100 = 900。本地事务提交前，先拿到该记录的全局锁，本地提交释放本地锁。tx2后开始，开启本地事务，拿到本地锁，更新操作m = 900 - 100 = 800。本地事务提交前，尝试拿到该记录的全局锁，tx1全局提交前，该记录的全局锁被tx1持有，tx2需要重试等待全局锁。
                二阶段全局提交，释放全局锁。tx2拿到全局锁，提交本地事务。
                    如果二阶段全局回滚，则tx1需要重新获取该数据的本地锁，进行反向补偿的更新操作，实现分支回滚。
                    此时，如果tx2仍在等待该数据的全局锁，同时持有本地锁，则tx1的分支回滚会失败。分支回滚会一直重试，直到tx2的全局锁等锁超时，放弃全局锁并回滚本地事务释放本地锁，tx1的分支回滚最终成功。
                    因为整个过程 全局锁在tx1结束前一直是被tx1持有的，所以不会发生脏写问题。
            2.读隔离
                在数据库本地事务隔离级别 读已提交或以上的基础上，Seata AT模式的默认全局隔离级别是读未提交。
                如果在特定场景下，必须要全局的读已提交，目前Seata方式是通过Select for update
                select for update 语句执行会申请 全局锁，如果全局锁被其他事务持有，则释放本地锁（回滚SELECT FOR UPDATE 语句的本地执行）并重试。这个过程中，查询是被block住的，直到全局锁拿到，即读取的相关数据是已提交的，才返回。
        分布式事务太复杂，导致系统吞吐量大幅度下跌。
            正常就系统A调用系统B，系统B调用系统C。调用报错打印异常日志。
            99%分布式接口调用，不要做分布式事务
            只需要监控调用报错（发邮件，发短信）记录日志（一旦出错，完整的日志）事后快速定位，排查和出解决方案，修复数据。
                比你做50个分布式事务的成本要低上百倍几十倍。
            要权衡
                要用分布式事务的时候，一定有成本，代码会很复杂，性能会下跌，系统更加复杂更加脆弱，更加容易出bug
                好处：做好了的话TCC，可靠消息最终一致性方案，一定可以100%保证你那块数据不会出错
    分布式事务的应用场景
        常见集群架构对分布式一致性问题的解决方案
            CP集群架构
                MySQL 主从同步复制集群
                ZooKeeper 集群
                Nacos CP模式
                Kafka 集群(ack=-1 或 ack = all)
                RocketMQ集群(同步刷盘模式)
            AP集群架构
                MySQL 主从异步复制集群
                MySQL 主从半同步复制集群
                Redis Cluster集群
                Redis 哨兵 + 主从集群
                RocketMQ集群(异步刷盘模式)
                Eureka Server 集群
                Nacos AP 模式
                Kafka集群(ack=0 或 ack = 1)
        实时的系统交互
            TCC的事务补偿--采用两阶段实现
        实时性要求不高
            异步通知
            可靠性消息
        跨平台
            最大努力通知型
            补偿+最大努力通知
    区别
        最终一致性
            最大努力通知和可靠消息的区别：
                业务场景不同
                解决思想不同
            本地消息表对比
                本地消息表与常规MQ对比
                    业务与消息发送的一致性区别
                        常规MQ队列消息    的处理流程无法实现
                        但是本地消息表通过同库双表可以实现
                可靠性消息和本地消息表对比
            TCC与Saga对比
        强一致性对比
            与2PC对比
                TCC与XA两阶段提交有什么区别？
                    强一致性和最终一致性区别
                    应用层面的不同
                        TCC的T本质上是应用层面的2PC
                2pc和3pc区别
                saga与2PC的区别
    成熟的解决方案
        复合框架seata
            seata和xa的区别
        最终一致性方案
            可靠消息
                只有RocketMQ支持可靠消息
            最大努力通知
                接收方结果查询的机制
                信息发送方最大努力通知的机制
            本地消息表方案
                理论和原理
                    在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中；
                    定时任务检测
                        转发成功？删表记录
                        转发失败？重试、补偿回滚
                优点
                    一种非常经典的实现，避免了分布式事务，实现了最终一致性。
                缺点
                    消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。
                核心问题
                    第二阶段的重试和幂等执行。失败后重试，这是一种补偿机制，它是能保证系统最终一致的关键流程。
            saga 补偿
                更多的作为一种思想进行使用
            TCC的框架
                hmily（TCC模式）
                ByteTCC（TCC模式）
                TCC-transaction
                seata的tcc堆springcloud没有提供支持
                TCC补偿模式
                    原理流程
                        1.Try 阶段主要是对业务系统做检测及资源预留
                        2.Confirm 阶段主要是对业务系统做确认提交，
                        3.Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。
                    三种异常处理
                        空回滚
                        幂等
                        悬挂
                    异步型tcc
                    tcc的特性
                        优点
                            跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些
                        缺点
                            Confirm 和 Cancel 失败重试，Confirm和Cancel接口必须实现幂等性
                            TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。
                            针对每个操作，都要注册响应的确认和核销操作，导致接口数量增加
                            对业务侵入性强，维护难度大
        强一致性的解决方案
            XA 两阶段提交(强一致)
                XA的特性
                    缺点
                        单节点问题
                            事务管理器在整个流程中扮演的角色很关键，如果其宕机;比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。
                        同步阻塞
                            在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源
                        数据不一致
                            两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能比如：在第二阶段中，假设协调者发出了事务 Commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 Commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。
                    优点
                封装框架
                    TX-LCN框架
                        锁定事务单元（lock）确认事务模块状态(confirm)通知事务(notify)
                        LCN模式
                        TCC模式
                        TXC模式
                2PC模式  二阶段协议
                    执行流程
                        阶段一：提交事务请求
                            1. 事务询问
                                协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各个参与者响应
                            2. 执行事务
                                各参与者节点执行事务操作，并将undo与redo信息写入事务日志中
                            3. 各参与者向协调者反馈事务询问的响应
                                如果参与者成功执行了事务，就返回YES，如果不成功，就返回NO。
                            该阶段相当于各个参与者对协调者发送的事务内容进行是否可以执行的投票
                        阶段二：执行事务提交
                            根据参与者的响应，正常情况下有两种情况
                                执行事务提交（响应都为YES）
                                    1. 发送提交请求
                                        协调者向所有参与者节点发出Commit请求
                                    2. 事务提交
                                        参与者接收到Commit请求后，会正式执行事务提交操作，并在完成后释放事务执行期间占用的资源
                                    3. 反馈事务提交结果
                                        参与者完成事务提交以后，向协调者发送Ack消息
                                    4. 完成事务
                                        收到所有参与者节点的Ack消息后，完成事务
                                中断事务（响应有NO，或有超时）
                                    1. 发送回滚请求
                                        协调者向所有参与者节点发出Rollback请求
                                    2. 事务回滚
                                        参与者接收到Rollback请求后，会根据undo信息执行事务回滚操作，并在完成后释放事务执行期间占用的资源
                                    3. 反馈事务回滚结果
                                        参与者完成事务回滚以后，向协调者发送Ack消息
                                    4. 中断事务
                                        收到所有参与者节点的Ack消息后，完成事务中断
                    优点
                        原理简单
                        实现方便
                    总结
                        使用分布式事务成本低，适用于数据库层面
                        在MySQL中不太理想
                        XA应用场景不理想
                            X/Opcn XA 接口
                    缺点
                        同步阻塞
                            在二段提交过程中，所有参与该事务操作的逻辑都处于阻塞状态，也就是各个参与者在等待其他参与者响应的过程中都无法执行其他操作
                        单点问题
                            协调者的角色在整个二段提交协议中起到了非常重要的作用，如果协调者出现问题，参与者将锁定事务资源无法继续完成事务操作。
                        数据不一致
                            在阶段二过程中，有可能因为网络等原因出现只有部分参与者收到了Commit请求，从而出现各个节点数据不一致的问题
                        太过保守
                            没有容错机制，任何一个节点的失败都会导致整个事务的中断
                3PC模式  三阶段协议
                    流程
                        一阶段：准备阶段（CanCommit）
                        二阶段：预提交阶段（PreCommit）
                        三阶段：提交阶段（DoCommit）
                    优点
                        降低了二阶段提交的阻塞范围
                    缺点
                        参与者收到preCommit消息后，一旦无法与协调者通信，将在超时后提交事务，在这种情况下，可能会出现数据的不一致性
                TCC的两阶段实现
    理论
        分布式一致性理论
            CAP
                三要素
                    一致性（Consistency）
                        分布式环境下，多个节点的数据是否强一致。
                    可用性（Availability）
                        分布式服务能一直保证可用状态。当用户发出一个请求后，服务能在有限时间内返回结果。
                    分区容错性（Partition Tolerance）
                        特指对网络分区的容忍性。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择
                3 个要素最多只能同时满足两个
                    AP
                        Cassandra
                        Dynamo
                    CP
                        HBase
                        MongoDB
            ACID
                Atomicity 原子性
                Consitency 一致性
                Isolation 隔离性
                Durability 持久性
            BASE理论
                基本可用(Basically Available)
                    实现基本可用的手段
                        流量削峰
                        延迟响应
                        体验降级
                            高清无码大图换成小图
                            服务降级
                        过载保护
                    指分布式系统在出现故障时，允许损失部分的可用性来保证核心可用；
                软状态(Soft state)
                    指允许分布式系统存在中间状态，该中间状态不会影响到系统的整体可用性；
                最终一致性(Eventual consistency)
                    实现最终一致性的手段
                        读时修复
                        写时修复
                        异步修复
                        基于消息中间件的最终一致性(消息中间件事务消息机制)
                原子性（A）与持久性（D）必须根本保障；
                为了可用性、性能与降级服务的需要，只有降低一致性( C ) 与 隔离性( I ) 的要求；
            多副本一致性与. DSM
                一致性模型
                    数据为中心的一致性模型
                    客户为中心的一致性模型
                分发协议
                    副本放置
                    更新传播
                一致性协议
                    主-从副本协议
                    复制写协议
                    高速缓存相关性协议
                分布式共享存储器
                    分布式共享存储器问题
                    基于页面的分布式共享存储器
                    共享变量的分布式共享存储器
                    基于对象的分布式共享存储器
        分布式一致性协议
            Paxos算法
            Paxos算法
                概述
                    一种基于消息传递具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一
                Paxos场景
                    古希腊有一种Paxos小岛，岛上采用一会的形式通过法令，议会中议员通过信使进行消息传递，议员与信使都是兼职的，他们随时都有可能会离开议会，并且信使有可能传递重复的信息，也有可能一去不复返，因此议会要保证在这种情况下法令仍然能够正确的产生，并且不会起冲突。
                问题描述
                    假设有一组可以提出提案的进程集合
                        对于一个一致性算法要保证一下几点
                            在这些提案中，只有一个被选定
                            如果没有提案被提出，就不会有选定的提案
                            当提案被选定以后，进程应该可以获取被选定的提案信息
                        对于一致性来说，安全性需求如下
                            只有被提出的提案才能被选定
                            只有一个提案被选定
                            如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个
                    该一致性算法中，有三种参与角色
                        Proposer（提议者）
                        Acceptor（决策者）
                        Learner（最终决策学习者）
                    一个参与者可能扮演多个角色，假设不同的参与者之间可以通过收发消息来进行通信
                        每个参与者以任意的速度执行，可能会因为出错而停止，也可能会重启
                        消息在传输过程中可能会出现不可预知的延迟，也有可能会重复或者丢失，但消息不会被损坏，即消息内容不能被篡改
                提案的选定
                    一个Acceptor
                        最简单的选定方式是只有一个Acceptor，Proposer发送给该Acceptor提案以后，Acceptor直接选择第一个提案为被选定的提案。但这种做法一旦Acceptor出问题，整个系统将无法正常工作。
                    多个Acceptor
                        Proposer向多个Acceptor集合发送提案，每个Acceptor都可能会批准（Accept）该提案，当足够多个Acceptor批准这个提案的时候，我们就认为该提案被选定了。
                        什么是足够多？
                            1. 足够多的集合是整个集合的子集
                            2. 这个集合大到包含Acceptor大多数成员
                            3. 每个Acceptor最多只能批准一个提案
                推导过程
                    概述
                        在没有失败和消息丢失的情况下，如果我们希望即使只有一个提案被提出，仍然可以选出一个提案，这就暗示了如下约束
                    约束
                        P1 ： 一个Acceptor必须批准他收到的第一个提案
                    问题
                        如果多个提案被不同的Proposer同时提出，这可能会导致虽然每个Acceptor都批准了他收到的第一个提案，但是没有一个提案是多个人批准的。也就是没有多数的Acceptor集合
                    规定
                        一个提案被选定需要被半数以上的Acceptor接受
                    解决
                        这个规定就暗示了在P1的基础上，一个Acceptor能够批准不止一个提案。我们使用全局的编号来唯一的标识每一个Acceptor批准的提案，当一个具有某value的提案被半数以上的Acceptor批准以后，我们就认为该value被选定。注意，提案和value不是一个概念，提案是由一个编号与value组成的结构体，因此我们用【编号，Value】来表示一个提案。
                        提案编号
                            给每个提案加上一个提案编号，表示提案被提出的顺序，不同的编号可以有相同的内容
                        Value
                            提案的内容
                    问题
                        虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的value值。否则又会出现不一致
                    约束
                        P2：如果编号为M,Value为V的提案（即【M，V】）被选定了，那么所有比M编号更高的，且被选定的提案，其Value值必须也是V。
                    P2需求
                        因为提案编号是全序的，P2就保证了只有一个Value值被选定这一关键安全性属性。同时，一个提案被选定，其首先必须被至少一个Acceptor批准，因此我们可以通过满足如下条件来满足P2
                    约束
                        P2a：如果编号为M,Value为V的提案（即【M，V】）被选定了，那么所有比编号M更高的，且Acceptor批准的提案，Value值必须也是V。
                    问题
                        假设总的有5个Acceptor。Proposer2提出[M1,V1]的提案，Acceptor2~5（半数以上）均接受了该提案，于是对于Acceptor2~5和Proposer2来讲，它们都认为V1被选定。Acceptor1刚刚从宕机状态恢复过来（之前Acceptor1没有收到过任何提案），此时Proposer1向Acceptor1发送了[M2,V2]的提案（V2≠V1且M2>M1），对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提案。）,Acceptor1必须接受该提案！同时Acceptor1认为V2被选定。这就出现了两个问题
                        1. Acceptor1认为V2被选定，Acceptor2~5和Proposer2认为V1被选定。出现了不一致。
                        2. V1被选定了，但是编号更高的被Acceptor1接受的提案[M2,V2]的value为V2，且V2≠V1。这就跟P2a（如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v）矛盾了。
                    约束
                        P2b：如果一个提案【M，V】被选定后，那么之后任何Proposer产生的编号更高的提案，其Value的值都为V。
                    问题
                        由P2b可推出P2a，进而推出P2。如何确保在某个Value为V的提案被选定后，Proposer提出的编号更高的提案的Value都是V呢？
                    约束
                        P2c：对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个
                            1. S中每个Acceptor都没有批准过编号小于N的提案
                            2. S中Acceptor接受过的最大编号的提案的value为V
                Proposer生成提案
                    概述
                        在P2C的条件下如何进行提案的生成。对于一个Proposer来说，获取那些已经通过的提案远比预测未来可能会通过的提案来的简单。因此Proposer在产生一个编号为M的提案时，必须要知道当前某一个将要或已经被半数以上Acceptor批准的编号小于M但未最大的编号的提案。并且，Proposer会要求所有Acceptor都不要批准任何编号小于M的提案。
                        也就是Proposer生成提案之前，应该先去『学习』已经被选定或者可能被选定的value，然后以该value作为自己提出的提案的value。如果没有value被选定，Proposer才可以自己决定value的值。这样才能达成一致。这个学习的阶段是通过一个『Prepare请求』实现的
                    提案生成算法
                        1. Proposer选择一个新的提案编号N，然后向某个Acceptor集合（半数以上）发送请求，要求该集合中的每个Acceptor做出如下响应（response），我们将该请求称为编号为N的Prepare请求
                             向Proposer承诺保证不再接受任何编号小于N的提案
                            如果Acceptor已经接受过提案，那么就向Proposer响应已经接受过的编号小于N的最大编号的提案
                        2. 如果Proposer收到了半数以上的Acceptor的响应，那么它就可以生成编号为N，Value为V的提案[N,V]。这里的V是所有的响应中编号最大的提案的Value。如果所有的响应中都没有提案，那 么此时V就可以由Proposer自己选择
                    生成提案后，Proposer将该提案发送给半数以上的Acceptor集合，并期望这些Acceptor能接受该提案。我们称该请求为Accept请求。（注意：此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合）
                Acceptor批准提案
                    概述
                        Acceptor可以忽略任何请求（包括Prepare请求和Accept请求）而不用担心破坏算法的安全性。因此，我们这里要讨论的是什么时候Acceptor可以响应一个请求
                    约束
                        P1a：一个Acceptor只要尚未响应过任何编号大于N的Prepare请求，那么他就可以接受这个编号为N的提案。
                算法优化
                    假设一个Acceptor收到了一个编号为N的Prepare请求，但此时该Acceptor已经对编号大于N的Prepare请求做出了响应，因此它肯定不会再批准任何新的编号为N的提案，那么很显然，Acceptor就没必要对这个Prepare请求作出响应，于是Acceptor可以选择忽略这样的Prepare请求。同时，Acceptor也可以忽略掉那些它已经批准过的提案的Prepare请求
                算法陈述
                    阶段一
                        1. Proposer选择一个提案编号M，然后向Acceptor的某个超过半数的子集成员发送编号为M的Prepare请求。
                        2. 如果一个Acceptor收到一个编号为M的Prepare请求，且编号M大于该Acceptor已经响应的所有Prepare请求的编号，那么它就会把已经批准过的最大的编号的提案作为相应反馈给Proposer，同时该Acceptor会承诺不会在批准任何编号小于M的提案。
                    阶段二
                        1. 如果Proposer收到来自半数以上的Acceptor对于其发出的编号为M的Prepare请求的响应，那么它就会发送一个针对【M，V】提案的Accepte请求给Acceptor。
注意：V的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么他就是任意值
                        2. 如果Acceptor收到的这个针对【M，V】的提案的Accept请求，只要该Acceptor尚未对编号大于M的Prepare请求作出响应，他就可以通过这个提案。
                提案的获取
                    方案一
                        一旦Acceptor批准了一个提案，就将该提案发送给所有的Learner
                    方案二
                        让所有的Acceptor将它们对提案的批准情况，统一发送给一个Learner，再由它通知其他的Learner
                    方案三
                        方案二的主节点存在单点问题，可以将主Leaner的范围扩大，即Acceptor可以将批准信息发送给一个特定的Learner集合，该集合中每个Learner都可以在一个提案被选定后通知其他Leaner
                通过选取主Proposer保证算法的活性
            RAFT(强领导者模型) paxos的下一代
                成员身份
                    Leader
                        处理写请求
                        管理日志复制(同步数据)
                        发送心跳
                    Follower
                        接收和处理来自领导者的消息
                        等待领导者心跳信息超时的时候，就主动站出来，推荐自己当候选人
                    Candidate
                        向其他节点发送请求投票信息，通知其他节点投票，如果赢得了半数以上选票，就晋升为Leader
            GOSSIP
                原理
                    Direct Mail 直接邮寄
                    Anti-entropy反熵
                        实现方式
                            推模式
                            拉模式
                            推拉模式
                    Rumor mongering谣言传播
            ZAB
                原理
                    成员身份
                        Leader
                            处理全部写请求
                            处理读请求
                        Follower
                            响应Leader心跳
                            参与领导者选举
                            处理读请求
                        Observer
                            不参与选举也不投票的Follower
                    成员状态
                        Looking：表明正在选举
                        Following：表明本节点是Follower节点
                        Leading：表明自己时Leader节点
                        Observing：表明自己是Observer节点
                    节点状态
                        Election：正在进行选举
                        Discovery：成员发现状态，确认领导关系
                        Synchronization：数据同步状态，以Leader节点数据为准，Follower节点修复数据副本的一致性
                        Broadcast：广播状态，集群各节点正常处理读写请求
                    Leader 选举
                    崩溃恢复
                        成员发现
                        数据同步
                    处理读写请求
                        读请求
                            与领导者失联的Follower，既不能处理写请求，也不能处理读请求
                            读请求可以在与Leader保持心跳的Follower上处理，实现的是最终一致性
                        写请求
                            写请求只能在领导节点上处理
                与Raft协议的对比
                    领导者选举
                        ZAB：见贤思齐，相互推荐
                        Raft：一张选票，先到先得，想要通讯的消息数更少，选举也更快
                    日志复制
                        两者相同，都是以领导者的日志为准来实现一致，而且日志必须是连续的，也必须按照顺序提交
                    读操作和一致性
                        ZAB：操作的顺序性，读操作最终一致性
                        Raft：既可提供强一致性，也可以提供最终一致性
                    写操作
                        两者相同。写操作必须在Leader节点上处理
                    其他
                        Raft设计更为简洁。节点发起选举后，递增任期编号，在选举结束后，广播心跳，直接建立领导者关系，然后向各节点同步日志，来实现数据副本的一致性。
        模型
            DTP模型
            一致性模型
                强一致性
                弱一致性
                最终一致性
                分布式系统数据的强一致性、弱一致性和最终一致性可以通过 Quorum NRW 算法分析。
    java中的事务
        JDBC事务
        JTA接口
            Atomikos实现
            JOTM实现