分布式 - 集群
    4.2 集群测试
        高可用集群
            keepalived
            虚拟ip地址
            2台nginx服务器
        4.2.1 集群概念
            集群优势
                高性能（Performance）
                价格有效性（Cost-effectiveness）
                可伸缩性（Scalability）
                高可用性（Availability）
                透明性（Transparency）
                可管理性（Manageability）
                可编程性（Programmability）
             集群分类
                常见分类
                    负载均衡集群（Load Balancing Clusters，LBC或者LB）
                    高可用性集群（High-Availability Clusters，HAC）
                    高性能计算集群（High-Performance Clusters，HPC
                    网格计算（Grid Computing）
                不同种类
                    负载均衡集群
                    高可用性集群
                    高性能计算集群
                    网格计算集群
             软硬选型
                硬件
                    F5、Netscaler、Radware、A10等，工作模式相当于Haproxy
                    缺点
                        成本高，不能二次开发
                    优点
                        性能强、更稳定
                软件
                    Nginx、LVS、Haproxy、Keepalived、Heartbeat
                    缺点
                        需要强有力的运维以及开发
                    优点
                        可以进行二次开发，互联网行业更倾向于开源软件
                选型
                    硬件
                        业务重要、技术薄弱、舍得开钱银行、证券、金融、宝马、奔驰
                    软件
                        利润较低的中小型企业
                    混合
                        门户网站淘宝、腾讯、新浪
                集群选型
                    中小互联网企业
                        负载均衡
                            Nginx
                                配置简单、使用方便、安全稳定、社区活跃、市场主流
                            Haproxy
                                支持L4/L7，社区活跃较低
                        高可用
                            Keeplived
                                部署快速、配置简单、使用方便、安全稳定
                            Heartbeat
                                使用较为复杂，经验到位的工程师可以进行选型
                    大型互联网企业
                        负载均衡
                            应用服务器
                                前端
                                    LVS+Keepalived 四层转发(主备或主主) 可拓展DNS或OSPF
                                后端
                                    Nginx 或 Haproxy 七层转发(支持百台级别)
                            数据库/存储
                                LVS+HeartbeatLVS支持TCP转发，DR模式高效，Heartbeat可以配合DRBD可以进行VIP切换，可以支持块设备级别的数据同步，可以支持资源服务管理
             集群均衡
                反向代理与负载均衡差异
                负载均衡组件
                并发调优参数
        4.2.2 部署测试
             集群规划
             集群部署
             集群调优
    数据库
        分库分表
            为什么要分库分表
                分库分表是两个概念，可能分库不分表，或者可能分表不分库
                1.部署Mqsql单机，扛不住并发。2.Mysql单机磁盘容量快满了3.Mysql单表数据量太大了，sql越跑越慢
                异步写系统，消费到了40万数据按照中的某个id，比如订单id进行hash，分发到对应数据库中。每个数据库单日增加40万数据。
                好处：1.MySql从单机->3机，承受并发增加了三倍。2.将原来的3千万数据，从一个库拆分到三个库，每个库就1/3的数据量，数据库服务器的磁盘使用率大大降低。3.原本一个蛋表是3千万数据，一个sql要花3秒钟去跑；拆分之后，每个库的每个表就1千万数据，一个SQL花1秒钟去跑就可以了
            用过哪些分库分表中间件？
                sharding-jdbc，mycat代理Proxy中间件：单独部署一个中间件客户端Client中间件：一个jar包让服务端直接用
                    sharding-jdbc
                        当当开源，属于client层方案。
                        sql语法支持比较多，没有太多限制
                        支持分库分表，读写分离，分布式id生成，柔性事务（最大努力送达型事务，TCC事务）
                        优点：是client层方案，不用部署，运维成本低，不需要代理层二次转发请求，性能很高。但是：如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合sharding-jdbc
                    mycat
                        基于cobar改造的，属于proxy层方案
                        支持的功能非常完善
                    如果在中小型公司选用sharding-jdbc，client层方案轻便，维护成本低，不需要额外增派人手。中大型最好选用mycat这类proxy层方案，团队很大，人员充足，那么最好是专门弄个人来研究和维护mycat，然后大量项目直接透明使用即可。
            如何进行数据库拆分
                垂直拆分
                    一个表有很多字段，可以拆成两个表，一个表放访问频率很高的字段，另一个表放访问频率比较低的字段。因为数据库是有缓存的，如果访问频率高的字段数据越少，在缓存里就能放更多的行，后续再访问的话效率就越好。
                水平拆分
                    把单表的数据拆成三份
                        还可以把每个表起不同的名字，然后再拆分
                    按照时间分发Range分发
                        一个月放到一个数据库里，然后每个星期对应数据库中分出的一张表。
                        好处：后面扩ring的时候，就很容易，因为只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写到新的库了。
                        缺点：大部分请求都是请求最新的数据，用户请求可能都到了最新的库里面去。实际生产用range要看场景。
                    按照ID分发（Hash分发）
                        orderId = 11，先根据数据库取模，11%3=2 分到数据库2，然后再根据数据库2中分的表取模 11%4=3，放到test_03
                        好处：可以平均每个库的数据量和请求压力。
                        坏处：扩容起来比较麻烦，新增一个数据库，需要重新hash取模。会有一个数据迁移的这么一个过程。
                    好处：单库最高承载2000/s的QPS，3个库最高可承载6000/s的QPS每个表的数据从原来的600万数据，缩小到50万数据。SQL的执行效率可能会增加好几倍。单库原来是600万数据，假设占据了600MB的磁盘空间。现在单库才200万数据，就仅仅占用了200M的磁盘空间
            如何把系统不停机迁移到分库分表
                假设你现在有一个单库单表的系统，在线上在跑，假设单表有600万数据，3个库，每个库分了4个表，每个表要放50万的数据。假设你已经选择了分库分表的中间件，你怎么把线上系统平滑的迁移到分库分表上面去？
                方案1:长时间停机分库分表
                    在网站上挂个公告，0-6点运维。
                    后台开一个临时程序，部署3台机器每台机器开20个线程，一个小时完成迁移
                    这个临时程序把数据给到数据库中间件，数据库中间件分发到多个数据库多个表中
                    然后修改主系统配置，让数据写入数据库中间件
                    缺点：1.一定会出现几个小时的停机。2.如果说到了凌晨3点没搞定，就慌了，到了凌晨五点还没搞定，就得回滚，继续单库单表，第二天凌晨继续搞
                方案2:不停机双写方案
                    1.修改系统中所有写库的代码，同时让他写老库和新分库分表的库。
                    2.开发一个后台数据临时迁移工具标准的规范化的表设计里面，都会包含最后修改的时间字段。判断一下分库分表中是否存在？不存在，直接写入。如果存在，比较两个数据的时间戳，如果比分库分表的数据要新，就覆盖分库分表里的数据
                    3.迁移完一轮，600万数据迁移完这么一轮，此时就需要执行一次检查。检查单库单表的数据，跟分库分表的数据是不是一致。如果一摸一样的话，那么就ok，迁移成功。如果不一样的话，针对不一样的数据，从单库单表中读取出来，看看是否需要再次覆盖分库分表中的数据。依次循环往复，这个后台程序你可能得跑个好几天，到了凌晨的时候，几乎没什么新的数据进来了，此时一般来说老库和新库的数据会变成一样的。
                    4.最后一步，修改系统的代码，将写单库单表的代码给删除掉，仅仅写分库分表，再次部署一下，就ok了
            如何设计动态扩容缩容的分库分表方案？
                1.停机扩容（不能用）
                    发公告停机
                    开发一个工具，把所有数据抽出来，重新进行分发写到新的库和表中去。
                    很不靠谱，从单库单表迁移到分库分表可能数据量并不是很大，但是如果已经分库分表了，说明数据量就很大了。
                2.优化后的方案
                    1.设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是32*32.
                        国内互联网公司肯定都是够用了
                        无论并发支撑还是数据量支撑都没问题
                        正常每个库的承载写入并发是1500，那么32个库就是48000的写并发，接近5万/s的写入并发，前面再加一个MQ，削峰，每秒写入MQ8万条，每秒消费5万条
                        假设每个表放500万条数据，在MySql里可以放50亿条数据。每秒5万并发，总共50亿数据。对于国内大部分互联网公司来说，其实一般都够了。
                    2.路由的规则：Id对32取模=库，Id/32再对32取模=表
                    3.刚开始四台服务器，每台服务器8个库。扩容的时候，申请增加更多的数据库服务器，装好mysql，倍数扩容，四台服务器，扩到8台服务器，16台服务器
                        注意：每台数据库服务器最大制成2000/s QPS。  扩容只需要新增四个数据库服务器，迁移四个库到新的数据库中。
                        直接迁移库这种方式，对于dba来说，他们都有对应的工具，方便和快捷很多
                        最多可以扩到32个服务器！每个数据库服务器上放一个库，这个库有32张表。
                    4.由dba负责将原先数据库服务器的库，迁移到新的数据库服务器上去，很多工具，库迁移比较便捷
                    5.我们这边只需要修改一下配置，调整迁移的库所在数据库服务器的地址
                    6.重新发布系统，上线，原先的路由规则变都不用变，直接可以基于2倍的数据库服务器的资源，继续进行线上系统的提供服务
                    好处：只是改变数据库服务器的数量，不用改变表的数量。而且动态缩容，可以改小数据库服务器的数量。扩容和缩容只需要改库的配置就可以了
            分库分表后，全局id怎么做呢
                1.单独搞一个生成主键的的库，全局就一个，每次插入数据，先往这个专门生成主键的库插入一条数据，拿到id再去写入分库中
                    好处：简单坏处：单库性能瓶颈
                    适合：并发很低，但是数据量很大的需求
                2.uuid
                    好处：本地生成，不用基于数据库来做了。坏处：uuid太长了，作为主键性能太差。
                3.获取系统当前时间
                    这个就是获取当前时间即可
                    但是并发很高的青枯杨，一秒并发几千，会有重复的情况，这个肯定不合适。基本上不用考虑了。
                    适合的场景：将时间和其他业务字段拼接起来，作为一个id，如果业务上可以接受也是可以的。比如时间戳+用户id+业务含义编码
                4.snowflake算法
                    会生成一个64位的long型的id，64位的long->二进制第一位：0 二进制首位0代表正数后面41位：放时间戳接下来5位：放机房id接下来5位：机器id最后：用来做序号
                    比如：两个机房，每个机房6台机器
                    机房2的第六台机器，想要在2021-01-01 10:00:00的时候，生成一个全局唯一的id   
                    它会往snowflake分布式id生成服务器一个请求，服务器就可以感知到:机房17 机器25 2021-01-01 10:00:00 
                        2021-01-01 10:00:00->处理后二进制换算41bit  0001100 10100010 10111110 10001001 01011100 00
                        机房id ，17 ->二进制 10001 
                        机器id，25 ->二进制 11001
                        snowflake算法服务，会判断一下，当前请求是否是机房17 机器25 在2021-01-01 10:00:00 的第一个请求如果是第一个请求，就全是0 ：0000 00000000
                        如果是是机房17 机器25 在2021-01-01 10:00:00发送了第二条消息，那么snowflake算法在这一毫秒，之前已经生成过一个id了，此时同一个机房同一个机器在同一毫秒还想再生成一个id，此时只能加1: 0000 00000001
                    代码思路
                        1.机房和机器都不能超过32个
                        2.每个机房的每个机器在每个毫秒内最大不能超过4096这个范围判断当前时间戳是否相等，如果相等sequence+1//刚开始第一次sequence是0，同一毫秒内就+1，然后进行二进制然后再记录一下当前生成的毫秒
                        3.将时间戳左移，放到41bit那；将机房左移放到5bit那；将机器id左移放到5bit那，将序号放到最后10bit；最后拼接成一个64bit的二进制数字，转成10进制就是个long型
        读写分离
            为什么要读写分离
                写请求：直接写入库，同时写入缓存
                读请求：先读缓存，如果缓存没有再去数据库里读
                什么情况下，缓存里会读不到数据？
                    1.缓存刚加上去，并没有把数据中的历史数据导入缓存。
                    2.缓存的内存塞满了，自动LRU了，删除了一些数据
                2000/s 数据库开始报警，磁盘io变慢，cpu负载过高，内存使用频率过高
            原理
                在主库写数据后，同步到从库，读的话去从库里读
                    一般挂3-5台从库
            如何实现
                MySQL原生就支持主从复制。主库->从库
                流程
                    1.Mysql有一个工作线程，写数据会把数据写到binlog日志里
                    2.从库会有一个io线程，会跟主库建立连接，主库也会有一个对应的io线程
                    3.主库io线程会把binlog日志读出来，然后传给从库io线程
                    4.从库的io线程会把收到的数据写入 relay日志
                        relay日志比较小，一般是存在os cache里，所以消耗的性能并不多
                    5.从库会有一个SQL线程，会不断从Relay日志里读数据，然后更新到自己本地的数据中
                    注意：从库 读取binlog日志，写relay日志，应用日志变更到自己本地数据库，都是串行化的。
                        从库的数据一般比主库要慢一些
                    说明一点：从库的IO线程，读取主库的binlog日志的时候，老版本是单线程的，5.6.x以后的新版本，也可以支持多线程读取
                        但是其他地方Mysql主库是多线程，但是从库还是单线程，还是会导致数据比主库慢一些
            主从同步延迟问题
                问题
                    延时：主要看主库的写并发，主库的写并发达到1000/s，从库的延时会有几ms；主库写并发达到2000/s，从库的延时可能会有几十ms；主库写并发达到4000/s，6000/s快死了，从库延时会达到几秒
                    宕机数据丢失：数据刚刚记录到主库中，主库如果突然宕机，从库切成主库进行写和读，但是主库中的一些数据还没同步到从库，可能丢失一些数据
                解决：
                    1.半同步复制semi-sync复制
                        指的是主库写入binlog日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的relay logo之后，接着会返回一个ack给主库，主库至少接收到一个从库的ack之后才会认为操作完成了。
                        如果主库没有接收到ack就宕机了，服务端就会收到数据并没有写入成功。
                    2.并行复制 mysql5.7新版本功能
                        从库开启多个SQL线程，每个线程从relay日志里读一个库的日志，然后并行重放不同库的日志，这是库级别的并行（缓解主从同步延时问题）
                    3.主从同步延时问题（精华）
                        show status， Seconds_Behind_Master 可以看从库复制主库的数据落后了几毫秒
                延迟导致生产环境的问题
                    插入一条数据，查出来这条数据，再更新这条数据
                        主从延迟导致更新失败
                    解决
                        一般来说，如果主从延迟较为严重
                        1.分库，一个主库拆为4个主库，每个库的并发500/s，这样，主从延迟可以忽略不计
                        2.打开mysql支持的并行复制，多个库并行复制，但是如果某个库的写入并发就是特别高，单库写并发达到2000/s。并行复制没意义。其实并行复制效果不一定很好。
                        3.重写代码，写代码的同学要慎重，插入数据之后，直接就更新，不用再查询这种
                        4.如果确实存在必须先插入，立马要求就查询到。可以用数据库中间件做到，直连主库查询。--不推荐这种方法，这么搞读写分离就没有意义了
                总结：
                    1.利用5.6版本的多个io线程
                    2.打开并行复制
                    3.如果还是不行并且QPS比较高，考虑分库
                    4.数据丢失问题，开半自动复制semi-sync